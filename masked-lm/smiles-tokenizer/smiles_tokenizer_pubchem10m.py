# -*- coding: utf-8 -*-
"""SMILES_Tokenizer_PubChem10M.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15G0iwuxsr4xrMFSnjXxfmU94utPn1Zvm

Requirements:
- !pip install transformers
- !pip install wandb
- !pip install nlp

PyTorch (latest should be fine)
"""

import transformers

import torch

import wandb
from transformers import RobertaConfig
from transformers import RobertaTokenizerFast
from transformers import RobertaForMaskedLM

from data_utils import RawTextDataset

from transformers import DataCollatorForLanguageModeling
from transformers import Trainer, TrainingArguments


# Main training script
wandb.login()

is_gpu = torch.cuda.is_available()

config = RobertaConfig(
    vocab_size=52_000,
    max_position_embeddings=512,
    num_attention_heads=12,
    num_hidden_layers=6,
    type_vocab_size=1,
)

tokenizer = RobertaTokenizerFast.from_pretrained("seyonec/SMILES_tokenized_PubChem_shard00_160k", max_len=512)


model = RobertaForMaskedLM(config=config)
model.num_parameters()

dataset = RawTextDataset(tokenizer=tokenizer, file_path="pubchem-10m.txt", block_size=512)


data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)


training_args = TrainingArguments(
    output_dir="PubChem_10M_SMILES_Tokenizer",
    overwrite_output_dir=True,
    num_train_epochs=10,
    per_device_train_batch_size=64,
    save_steps=10_000,
    save_total_limit=2,
    fp16 = is_gpu, # fp16 only works on CUDA devices
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)


trainer.train()
trainer.save_model("PubChem_10M_SMILES_Tokenizer")

"""# Methods for preventing RuntimeError

- reduce per_gpu_train_batch_size to 16
- reduce seq_length to 128 or below (recommended to be 128 minimum just in case)
"""